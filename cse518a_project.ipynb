{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridworldMdp\n",
    "from agents import OptimalAgent, MyopicAgent, UncalibratedAgent\n",
    "from mdp_interface import Mdp\n",
    "from agent_runner import get_reward_from_trajectory, run_agent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recognized-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Intervention:\n",
    "\n",
    "    def __init__(self,trial_length=10,num_interventions=3,gamma=0.9):\n",
    "        self.steps_left = trial_length\n",
    "        self.interventions_left = num_interventions\n",
    "        self.optimal_agent = OptimalAgent(gamma=gamma)\n",
    "\n",
    "    def set_mdp(self,mdp):\n",
    "        self.optimal_agent.set_mdp(mdp)\n",
    "\n",
    "    def get_optimal_action(self,state):\n",
    "        return self.optimal_agent.get_action(state)\n",
    "\n",
    "    def will_intervene(self,state,agent):\n",
    "        raise NotImplemented(\"Cannot call will_intervene for Intervention\")\n",
    "\n",
    "    def get_action(self,state,agent):\n",
    "        if self.will_intervene(state,agent):\n",
    "            self.interventions_left -= 1\n",
    "            self.steps_left -= 1\n",
    "            return self.get_optimal_action(state)\n",
    "        self.steps_left -= 1\n",
    "        return agent.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exact-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomIntervention(Intervention):\n",
    "\n",
    "    def will_intervene(self, state, agent):\n",
    "        prob = self.interventions_left / self.steps_left\n",
    "        return np.random.rand() < prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dying-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicIntervention(Intervention):\n",
    "\n",
    "    def __init__(self, trial_length=10, num_interventions=3, gamma=0.9, qval_threshold=2):\n",
    "        super().__init__(trial_length=trial_length, num_interventions=num_interventions, gamma=gamma)\n",
    "        self.qval_threshold = qval_threshold\n",
    "\n",
    "    def will_intervene(self, state, agent):\n",
    "        agent_action = agent.get_action()\n",
    "        optimal_action = self.get_optimal_action()\n",
    "        mu = self.optimal_agent.extend_state_to_mu(state)\n",
    "        agent_qval = self.optimal_agent.qvalue(mu,agent_action)\n",
    "        optimal_qval = self.optimal_agent.qvalue(mu,optimal_action)\n",
    "        return optimal_qval - agent_qval > self.qval_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "stuffed-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "height=7\n",
    "width=7\n",
    "num_rewards=4\n",
    "\n",
    "def gen_random_connected():\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            return GridworldMdp.generate_random_connected(height=height,width=width,num_rewards=num_rewards,noise=0)\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError('Could not generate Gridworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "equivalent-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1000\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tropical-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(agent, intervention, trial_length):\n",
    "\n",
    "    mdp = gen_random_connected()\n",
    "    env = Mdp(mdp)\n",
    "    agent.set_mdp(mdp)\n",
    "    intervention.set_mdp(mdp)\n",
    "    trajectory = []\n",
    "\n",
    "    for _ in range(trial_length):\n",
    "        curr_state = env.get_current_state()\n",
    "        action = intervention.get_action(curr_state,agent)\n",
    "        next_state, reward = env.perform_action(action)\n",
    "        minibatch = (curr_state, action, next_state, reward)\n",
    "        agent.inform_minibatch(*minibatch)\n",
    "        trajectory.append(minibatch)\n",
    "\n",
    "    reward = get_reward_from_trajectory(trajectory,gamma)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gorgeous-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_mdp(mdp):\n",
    "    size = height * width\n",
    "    arr = np.zeros(size * 2, dtype=np.int8)\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            if mdp.walls[y][x]:\n",
    "                arr[x * width + y] = 1\n",
    "            elif (x,y) in mdp.rewards:\n",
    "                arr[size + x * width + y] = mdp.rewards[(x,y)]\n",
    "            elif (x,y) == mdp.get_start_state():\n",
    "                arr[x * width + y] = -1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "charming-revelation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterI 9\n",
      "Intervention Helped: 145/200 times\n",
      "CPU times: user 3.37 s, sys: 24 ms, total: 3.39 s\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_grids = 10\n",
    "num_start_states = 20\n",
    "episode_length = 10\n",
    "\n",
    "def gen_data():\n",
    "    agent = MyopicAgent(horizon=2)\n",
    "    optimal_agent = OptimalAgent()\n",
    "    total_trials = num_grids * num_start_states\n",
    "    mdp_size = width * height * 2\n",
    "    data = np.zeros((total_trials,mdp_size+2))\n",
    "    trial = 0\n",
    "\n",
    "    # number of times when intervention actually made a difference\n",
    "    count = 0\n",
    "    total_actions = 0\n",
    "    diff_actions = 0\n",
    "\n",
    "    for i in range(num_grids):\n",
    "        print(f'IterI {i}', end='\\r')\n",
    "        gridworld = gen_random_connected()\n",
    "        mdp = Mdp(gridworld)\n",
    "        # flat_gridworld = flatten_mdp(gridworld)\n",
    "        # perform intervention with various episode lengths left\n",
    "        for j in range(num_start_states):\n",
    "            #print(f'IterJ: {j}')\n",
    "            start_state = gridworld.get_random_start_state()\n",
    "            mdp.gridworld.start_state = start_state\n",
    "            #print(mdp.gridworld, end='\\n\\n')\n",
    "            \n",
    "            agent.set_mdp(gridworld)\n",
    "            optimal_agent.set_mdp(gridworld)\n",
    "            \n",
    "            agent_action = agent.get_action(start_state)\n",
    "            optimal_action = optimal_agent.get_action(start_state)\n",
    "            #print(f'Start State: {start_state}')\n",
    "            #print(f'Agent Action: {agent_action}')\n",
    "            #print(f'Optimal Action: {optimal_action}')\n",
    "            \n",
    "            total_actions += 1\n",
    "            if agent_action != optimal_action:\n",
    "                diff_actions+=1\n",
    "                \n",
    "                #print(mdp.gridworld, end='\\n\\n')\n",
    "                #action_res = mdp.perform_action(agent_action)\n",
    "                #print(action_res)\n",
    "                #print(mdp.gridworld, end='\\n\\n')\n",
    "                agent_trajectory = run_agent(agent,mdp,episode_length=episode_length)\n",
    "                r1 = get_reward_from_trajectory(agent_trajectory)\n",
    "                mdp.gridworld.start_state = start_state\n",
    "                \n",
    "                \n",
    "                #print(mdp.gridworld, end='\\n\\n')\n",
    "                #action_res = mdp.perform_action(optimal_action)\n",
    "                #print(action_res)\n",
    "                #print(mdp.gridworld, end='\\n\\n')\n",
    "                intervened_trajectory = run_agent(agent,mdp,episode_length=episode_length, first_optimal=optimal_agent)\n",
    "                r2 = get_reward_from_trajectory(intervened_trajectory)\n",
    "                \n",
    "                #for t1,t2 in zip(agent_trajectory, intervened_trajectory): print(f'{t1}\\t|\\t{t2}')\n",
    "                \n",
    "            #data[trial,:mdp_size] = flat_mdp\n",
    "            #x,y = start_state\n",
    "            #data[trial,-2] = y * width + x\n",
    "            #data[trial,-1] = r2 - r1\n",
    "\n",
    "            #print(f'Trajectory: {len(trajectory)}')\n",
    "            #for t in trajectory:\n",
    "            #    print(t)\n",
    "                \n",
    "                #if r1!=r2: print(f'Rewards: ##########{r1},{r2}#########')\n",
    "                #else: print(f'Rewards: {r1},{r2}')\n",
    "                if r1 != r2:\n",
    "                    count += 1\n",
    "            trial += 1\n",
    "            #print('---------------------------------------')\n",
    "    print()\n",
    "    print(f'Intervention Helped: {count}/{num_grids*num_start_states} times')\n",
    "    #print(total_actions, diff_actions)\n",
    "    return data\n",
    "\n",
    "data = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-overhead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "moral-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXX 0\n",
      "X 2X\n",
      "X AX\n",
      "XXXX\n",
      "\n",
      "Iter: 0\n",
      "Start State: (1, 1)\n",
      "Optimal Action: (1, 0)\n",
      "Trajectory: 10\n",
      "((2, 2), (0, -1), (2, 1), -0.01)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "((2, 1), (0, 0), (2, 1), 2.0)\n",
      "Rewards: 11.016431198000001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mgen_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r1' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_grids = 1\n",
    "num_start_states = 20\n",
    "episode_length = 10\n",
    "\n",
    "def gen_data():\n",
    "    optimal_agent = OptimalAgent()\n",
    "    total_trials = num_grids * num_start_states\n",
    "    mdp_size = width * height * 2\n",
    "    trial = 0\n",
    "\n",
    "    # number of times when intervention actually made a difference\n",
    "    cnt = 0\n",
    "    total_actions = 0\n",
    "\n",
    "    for i in range(num_grids):\n",
    "        print(f'Iter {i}', end='\\r')\n",
    "        mdp = gen_random_connected()\n",
    "        env = Mdp(mdp)\n",
    "        optimal_agent.set_mdp(mdp)\n",
    "        print(mdp, end='\\n\\n')\n",
    "        # perform intervention with various episode lengths left\n",
    "        for j in range(num_start_states):\n",
    "            print(f'Iter: {j}')\n",
    "            start_state = mdp.get_random_start_state()\n",
    "            env.state = start_state\n",
    "            optimal_action = optimal_agent.get_action(start_state)\n",
    "            r2 = 0.0\n",
    "            total_actions += 1\n",
    "            if True:\n",
    "                print(f'Start State: {start_state}')\n",
    "                print(f'Optimal Action: {optimal_action}')\n",
    "                env.perform_action(optimal_action)\n",
    "                trajectory = run_agent(optimal_agent,env,episode_length=episode_length)\n",
    "                r2 = get_reward_from_trajectory(trajectory)\n",
    "            print(f'Trajectory: {len(trajectory)}')\n",
    "            for t in trajectory:\n",
    "                print(t)\n",
    "            print(f'Rewards: {r2}')\n",
    "            if r1 != r2:\n",
    "                cnt += 1\n",
    "            trial += 1\n",
    "            print('---------------------------------------')\n",
    "    print()\n",
    "    print(cnt)\n",
    "    print(total_actions, diff_actions)\n",
    "\n",
    "gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-convergence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-message",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-patch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-government",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-singapore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-omaha",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-country",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed64e5a8ec65e8575703d495269facee7e989d76102892ef4c34bce8b4483af7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
