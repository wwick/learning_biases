{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd92f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridworldMdp\n",
    "from agents import OptimalAgent, MyopicAgent, UncalibratedAgent\n",
    "from mdp_interface import Mdp\n",
    "from agent_runner import get_reward_from_trajectory, run_agent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01d9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c8d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gridworld_arr(gridworld):\n",
    "    size = height * width\n",
    "    arr = np.zeros((3,width,height), dtype=np.int8)\n",
    "    arr[0] = np.array(gridworld.walls)\n",
    "    \n",
    "    for (x,y) in gridworld.rewards:\n",
    "        arr[1,x,y] = gridworld.rewards[(x,y)]\n",
    "        \n",
    "    (x,y) = gridworld.get_start_state()\n",
    "    arr[2,x,y] = 1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2225fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_connected(height, width, num_rewards):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            return GridworldMdp.generate_random_connected(height=height,width=width,num_rewards=num_rewards,noise=0)\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError('Could not generate Gridworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4912bfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterI 9999\n",
      "(10000, 4, 7, 7)\n",
      "Out of 10000 actions, 7209 were different\n",
      "CPU times: user 3min 58s, sys: 2.66 s, total: 4min\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "height=7\n",
    "width=7\n",
    "num_rewards=4\n",
    "\n",
    "num_trials = 1\n",
    "gamma = 0.9\n",
    "\n",
    "num_grids = 10000\n",
    "num_start_states = 1\n",
    "episode_length = 10\n",
    "\n",
    "def gen_data():\n",
    "    agent = MyopicAgent(horizon=2)\n",
    "    optimal_agent = OptimalAgent()\n",
    "    total_trials = num_grids * num_start_states\n",
    "    mdp_size = width * height * 2\n",
    "    data = np.zeros((total_trials,4,width,height))\n",
    "    trial = 0\n",
    "\n",
    "    # number of times when intervention actually made a difference\n",
    "    count = 0\n",
    "    total_actions = 0\n",
    "    diff_actions = 0\n",
    "\n",
    "    for i in range(num_grids):\n",
    "        print(f'IterI {i}', end='\\r')\n",
    "        gridworld = gen_random_connected(height, width, num_rewards)\n",
    "        mdp = Mdp(gridworld)\n",
    "        gridworld_arr = gen_gridworld_arr(gridworld)\n",
    "        # perform intervention with various episode lengths left\n",
    "        for j in range(num_start_states):\n",
    "            start_state = gridworld.get_random_start_state()\n",
    "            mdp.gridworld.start_state = start_state\n",
    "            #print(mdp.gridworld, end='\\n\\n')\n",
    "            \n",
    "            agent.set_mdp(gridworld)\n",
    "            optimal_agent.set_mdp(gridworld)\n",
    "            \n",
    "            agent_action = agent.get_action(start_state)\n",
    "            optimal_action = optimal_agent.get_action(start_state)\n",
    "            \n",
    "            r1,r2 = 0.0,0.0\n",
    "            \n",
    "            total_actions += 1\n",
    "            if agent_action != optimal_action:\n",
    "                diff_actions+=1\n",
    "                \n",
    "                agent_trajectory = run_agent(agent,mdp,episode_length=episode_length)\n",
    "                r1 = get_reward_from_trajectory(agent_trajectory)\n",
    "                intervened_trajectory = run_agent(agent,mdp,episode_length=episode_length, first_optimal=optimal_agent)\n",
    "                r2 = get_reward_from_trajectory(intervened_trajectory)\n",
    "                \n",
    "#             print(mdp_size, len(gridworld_arr))\n",
    "#             print(gridworld_arr)\n",
    "#             print('rewards', r1 ,r2)\n",
    "            data[trial,:3] = gridworld_arr\n",
    "            data[trial,3] = r2 - r1\n",
    "\n",
    "            trial += 1\n",
    "            #print('---------------------------------------')\n",
    "#     print()\n",
    "#     print(data)\n",
    "    print()\n",
    "    print(data.shape)\n",
    "    print(f'Out of {total_actions} actions, {diff_actions} were different')\n",
    "    return data\n",
    "\n",
    "data = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbddc698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data[:,3,0,0]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e910d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2803"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y==0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ce86e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2389"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y>0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b110b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4808"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y<0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a2824a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.379641657349001"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b9c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60911898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334ec9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed64e5a8ec65e8575703d495269facee7e989d76102892ef4c34bce8b4483af7"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
