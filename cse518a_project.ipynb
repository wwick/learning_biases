{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8531a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"Defines the interface that an agent playing an MDP should implement.\"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0):\n",
    "        \"\"\"Initializes the agent, setting any relevant hyperparameters.\"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.current_discount = 1.0\n",
    "        self.reward = 0.0\n",
    "\n",
    "    def set_mdp(self, mdp):\n",
    "        \"\"\"Sets the MDP that the agent will be playing.\"\"\"\n",
    "        self.mdp = mdp\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Returns the action that the agent takes in the given state.\n",
    "        The agent should imagine that it is in the given state when selecting an\n",
    "        action. When the agent is actually acting in an environment, the\n",
    "        environment will guarantee that it always passes in the current state of\n",
    "        the agent. However, for other purposes, sequential calls to `get_action`\n",
    "        are not required to be part of the same trajectory.\n",
    "        state: State of the agent. An element of self.mdp.get_states().\n",
    "        Returns: An action a such that a is in self.mdp.get_actions(state).\n",
    "        \"\"\"\n",
    "        return self.get_action_distribution(state).sample()\n",
    "\n",
    "    def get_action_distribution(self, state):\n",
    "        \"\"\"Returns a Distribution over actions that the agent takes in `state`.\n",
    "        The agent should imagine that it is in the given state when selecting an\n",
    "        action. When the agent is actually acting in an environment, the\n",
    "        environment will guarantee that it always passes in the current state of\n",
    "        the agent. However, for other purposes, sequential calls to\n",
    "        `get_action_distribution` are not required to be part of the same\n",
    "        trajectory.\n",
    "        state: State of the agent. An element of self.mdp.get_states().\n",
    "        Returns: A Distribution over actions.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"get_action_distribution not implemented\")\n",
    "\n",
    "    def inform_minibatch(self, state, action, next_state, reward):\n",
    "        \"\"\"Updates the agent based on the results of the last action.\"\"\"\n",
    "        self.reward += self.current_discount * reward\n",
    "        self.current_discount *= self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048bb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationLikeAgent(Agent):\n",
    "    \"\"\"An agent that chooses actions using something similar to value iteration.\n",
    "    Instead of working directly on states from the mdp, we perform value\n",
    "    iteration on generalized states (called mus), following the formalism in\n",
    "    \"Learning the Preferences of Bounded Agents\" from a NIPS 2015 workshop.\n",
    "    In the default case, a single MDP provides all of the necessary\n",
    "    information. However, to support evaluation of reward learning, you can\n",
    "    optionally specify a reward_mdp in the set_mdp method, in which case all\n",
    "    reward evaluations will be done by the reward_mdp (while everything else\n",
    "    such as transition probabilities will still use the original MDP).\n",
    "    The algorithm in this class is simply standard value iteration, but\n",
    "    subclasses can easily change the behavior while reusing most of the code by\n",
    "    overriding hooks into the algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0.9, beta=None, num_iters=50):\n",
    "        \"\"\"Initializes the agent, setting any relevant hyperparameters.\n",
    "        gamma: Discount factor.\n",
    "        beta: Noise parameter when choosing actions. beta=None implies that\n",
    "        there is no noise, otherwise actions are chosen with probability\n",
    "        proportional to exp(beta * value).\n",
    "        num_iters: The maximum number of iterations of value iteration to run.\n",
    "        \"\"\"\n",
    "        super(ValueIterationLikeAgent, self).__init__(gamma)\n",
    "        self.beta = beta\n",
    "        self.num_iters = num_iters\n",
    "        self.policy = None\n",
    "\n",
    "    def set_mdp(self, mdp, reward_mdp=None):\n",
    "        super(ValueIterationLikeAgent, self).set_mdp(mdp)\n",
    "        self.reward_mdp = reward_mdp if reward_mdp is not None else mdp\n",
    "        self.compute_values()\n",
    "\n",
    "    def compute_values(self):\n",
    "        \"\"\"Computes the values for self.mdp using value iteration.\n",
    "        Populates an object self.values, such that self.values[mu] is the value\n",
    "        (a float) of the generalized state mu.\n",
    "        \"\"\"\n",
    "        values = defaultdict(float)\n",
    "        for iter in range(self.num_iters):\n",
    "            new_values = defaultdict(float)\n",
    "            for mu in self.get_mus():\n",
    "                actions = self.get_actions(mu)\n",
    "                if not actions:\n",
    "                    continue\n",
    "                new_mu = self.get_mu_for_planning(mu)  # Typically new_mu == mu\n",
    "                qvalues = [(self.qvalue(new_mu, a, values), a) for a in actions]\n",
    "                _, chosen_action = max(qvalues)\n",
    "                new_values[mu] = self.qvalue(mu, chosen_action, values)\n",
    "\n",
    "            if self.converged(values, new_values):\n",
    "                self.values = new_values\n",
    "                return\n",
    "\n",
    "            values = new_values\n",
    "\n",
    "        self.values = values\n",
    "\n",
    "    def converged(self, values, new_values, tolerance=1e-3):\n",
    "        \"\"\"Returns True if value iteration has converged.\n",
    "        Value iteration has converged if no value has changed by more than tolerance.\n",
    "        values: The values from the previous iteration of value iteration.\n",
    "        new_values: The new value computed during this iteration.\n",
    "        \"\"\"\n",
    "        for mu in new_values.keys():\n",
    "            if abs(values[mu] - new_values[mu]) > tolerance:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def value(self, mu):\n",
    "        \"\"\"Computes V(mu).\n",
    "        mu: Generalized state\n",
    "        \"\"\"\n",
    "        return self.values[mu]\n",
    "\n",
    "    def qvalue(self, mu, a, values=None):\n",
    "        \"\"\"Computes Q(mu, a) from the values table.\n",
    "        mu: Generalized state\n",
    "        a: Action\n",
    "        values: Dictionary such that values[mu] is the value of generalized\n",
    "        state mu. If None, then self.values is used instead.\n",
    "        \"\"\"\n",
    "        if values is None:\n",
    "            values = self.values\n",
    "        r = self.get_reward(mu, a)\n",
    "        transitions = self.get_transition_mus_and_probs(mu, a)\n",
    "        return r + self.gamma * sum([p * values[mu2] for mu2, p in transitions])\n",
    "\n",
    "    def get_action_distribution(self, s):\n",
    "        \"\"\"Returns a Distribution over actions.\n",
    "        Note that this is a normal state s, not a generalized state mu.\n",
    "        \"\"\"\n",
    "        mu = self.extend_state_to_mu(s)\n",
    "        actions = self.mdp.get_actions(s)\n",
    "        if self.beta is not None:\n",
    "            q_vals = np.array([self.qvalue(mu, a) for a in actions])\n",
    "            q_vals = q_vals - np.mean(q_vals)  # To prevent overflow in exp\n",
    "            action_dist = np.exp(self.beta * q_vals)\n",
    "            return Distribution(dict(zip(actions, action_dist)))\n",
    "\n",
    "        best_value, best_actions = float(\"-inf\"), []\n",
    "        for a in actions:\n",
    "            action_value = self.qvalue(mu, a)\n",
    "            if action_value > best_value:\n",
    "                best_value, best_actions = action_value, [a]\n",
    "            elif action_value == best_value:\n",
    "                best_actions.append(a)\n",
    "        return Distribution({a : 1 for a in best_actions})\n",
    "        # For more determinism, you can break ties deterministically:\n",
    "        # return Distribution({best_actions[0] : 1})\n",
    "\n",
    "    def get_mus(self):\n",
    "        \"\"\"Returns all possible generalized states the agent could be in.\n",
    "        This is the equivalent of self.mdp.get_states() for generalized states.\n",
    "        \"\"\"\n",
    "        return self.mdp.get_states()\n",
    "\n",
    "    def get_actions(self, mu):\n",
    "        \"\"\"Returns all actions the agent could take from generalized state mu.\n",
    "        This is the equivalent of self.mdp.get_actions() for generalized states.\n",
    "        \"\"\"\n",
    "        s = self.extract_state_from_mu(mu)\n",
    "        return self.mdp.get_actions(s)\n",
    "\n",
    "    def get_reward(self, mu, a):\n",
    "        \"\"\"Returns the reward for taking action a from generalized state mu.\n",
    "        This is the equivalent of self.mdp.get_reward() for generalized states.\n",
    "        \"\"\"\n",
    "        s = self.extract_state_from_mu(mu)\n",
    "        return self.reward_mdp.get_reward(s, a)\n",
    "\n",
    "    def get_transition_mus_and_probs(self, mu, a):\n",
    "        \"\"\"Gets information about possible transitions for the action.\n",
    "        This is the equivalent of self.mdp.get_transition_states_and_probs() for\n",
    "        generalized states. So, it returns a list of (next_mu, prob) pairs,\n",
    "        where next_mu must be a generalized state.\n",
    "        \"\"\"\n",
    "        s = self.extract_state_from_mu(mu)\n",
    "        return self.mdp.get_transition_states_and_probs(s, a)\n",
    "\n",
    "    def get_mu_for_planning(self, mu):\n",
    "        \"\"\"Returns the generalized state that an agent uses for planning.\n",
    "        Specifically, the returned state is used when looking forward to find\n",
    "        the expected value of a future state.\n",
    "        \"\"\"\n",
    "        return mu\n",
    "\n",
    "    def extend_state_to_mu(self, state):\n",
    "        \"\"\"Converts a normal state to a generalized state.\"\"\"\n",
    "        return state\n",
    "\n",
    "    def extract_state_from_mu(self, mu):\n",
    "        \"\"\"Converts a generalized state to a normal state.\"\"\"\n",
    "        return mu\n",
    "\n",
    "class OptimalAgent(ValueIterationLikeAgent):\n",
    "    \"\"\"An agent that implements regular value iteration.\"\"\"\n",
    "    def __str__(self):\n",
    "        pattern = 'Optimal-gamma-{0.gamma}-beta-{0.beta}-numiters-{0.num_iters}'\n",
    "        return pattern.format(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6767e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldMdpNoR(object):\n",
    "    \"\"\"A grid world where the objective is to navigate to one of many rewards.\n",
    "    Specifies all of the static information that an agent has access to when\n",
    "    playing in the given grid world, including the state space, action space,\n",
    "    transition probabilities, start state, etc. The agent can take any of the \\\n",
    "    four cardinal directions as an action, or the STAY action.\n",
    "    The reward is by default *not present*, though subclasses may add in\n",
    "    funcitonality for the reward.\n",
    "    \"\"\"\n",
    "    def __init__(self, walls, start_state, noise=0):\n",
    "        self.height = len(walls)\n",
    "        self.width = len(walls[0])\n",
    "        self.walls = walls\n",
    "        self.start_state = start_state\n",
    "        self.noise = noise\n",
    "        self.transition_matrix = None\n",
    "\n",
    "    def get_start_state(self):\n",
    "        \"\"\"Returns the start state.\"\"\"\n",
    "        return self.start_state\n",
    "\n",
    "    def get_states(self):\n",
    "        \"\"\"Returns a list of all possible states the agent can be in.\n",
    "        Note it is not guaranteed that the agent can reach all of these states.\n",
    "        \"\"\"\n",
    "        coords = [(x, y) for x in range(self.width) for y in range(self.height)]\n",
    "        all_states = [(x, y) for x, y in coords if not self.walls[y][x]]\n",
    "        return all_states\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        \"\"\"Returns the list of valid actions for 'state'.\n",
    "        Note that you can request moves into walls, which are equivalent to\n",
    "        STAY. The order in which actions are returned is guaranteed to be\n",
    "        deterministic, in order to allow agents to implement deterministic\n",
    "        behavior.\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        if self.walls[y][x]:\n",
    "            raise ValueError('Cannot be inside a wall!')\n",
    "        return [Direction.NORTH, Direction.SOUTH, Direction.EAST, Direction.WEST, Direction.STAY]\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"Get reward for state, action transition.\"\"\"\n",
    "        raise NotImplemented(\"Cannot call get_reward for GridworldMdpNoR\")\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return False\n",
    "\n",
    "    def get_transition_states_and_probs(self, state, action):\n",
    "        \"\"\"Gets information about possible transitions for the action.\n",
    "        Returns list of (next_state, prob) pairs representing the states\n",
    "        reachable from 'state' by taking 'action' along with their transition\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        if action not in self.get_actions(state):\n",
    "            raise ValueError(\"Illegal action %s in state %s\" % (action, state))\n",
    "\n",
    "        if action == Direction.STAY:\n",
    "            return [(state, 1.0)]\n",
    "\n",
    "        next_state = self._attempt_to_move_in_direction(state, action)\n",
    "        if self.noise == 0.0:\n",
    "            return [(next_state, 1.0)]\n",
    "\n",
    "        successors = defaultdict(float)\n",
    "        successors[next_state] += 1.0 - self.noise\n",
    "        for direction in Direction.get_adjacent_directions(action):\n",
    "            next_state = self._attempt_to_move_in_direction(state, direction)\n",
    "            successors[next_state] += (self.noise / 2.0)\n",
    "\n",
    "        return successors.items()\n",
    "\n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"Returns transition matrix. Very slow.\"\"\"\n",
    "        if self.noise != 0:\n",
    "            raise AssertionError(\"Transition matrix does not have computations set when MDP has noise\")\n",
    "        if self.transition_matrix != None:\n",
    "            return self.transition_matrix\n",
    "\n",
    "        height = self.height\n",
    "        width = self.width\n",
    "        num_actions = len(Direction.ALL_DIRECTIONS)\n",
    "\n",
    "        tran_shape = (width*height, num_actions, width*height)\n",
    "        transition_matrix = np.zeros(tran_shape)\n",
    "\n",
    "        # Init the array to stay action, even if in wall\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                flatOuter = y * width + x\n",
    "\n",
    "                for idxA, action in enumerate(Direction.ALL_DIRECTIONS):\n",
    "                    # Stay action is default for every state, even walls\n",
    "                    transition_matrix[flatOuter, idxA, flatOuter] = 1\n",
    "\n",
    "                    # Compute s,a -> s' transitions\n",
    "                    try:\n",
    "                        # self.get_actions(self, state) <-- takes state in non-gridworld format\n",
    "                        # of (x, y)\n",
    "                        sa_transitions = self.get_transition_states_and_probs((x, y), action)\n",
    "                    except ValueError:\n",
    "                        sa_transitions = None\n",
    "\n",
    "                    if sa_transitions:\n",
    "                        transition_matrix[flatOuter, idxA, flatOuter] = 0\n",
    "                        for state, prob in sa_transitions:\n",
    "                            flatInner = state[1] * width + state[0]\n",
    "                            transition_matrix[flatOuter, idxA, flatInner] = prob\n",
    "\n",
    "        self.transition_matrix = transition_matrix\n",
    "        return self.transition_matrix\n",
    "\n",
    "    def _attempt_to_move_in_direction(self, state, action):\n",
    "        \"\"\"Return the new state an agent would be in if it took the action.\n",
    "        Requires: action is in self.get_actions(state).\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        newx, newy = Direction.move_in_direction(state, action)\n",
    "        return state if self.walls[newy][newx] else (newx, newy)\n",
    "\n",
    "\n",
    "\n",
    "class GridworldMdp(GridworldMdpNoR):\n",
    "    \"\"\"A grid world where the objective is to navigate to one of many rewards.\n",
    "    Specifies all of the static information that an agent has access to when\n",
    "    playing in the given grid world, including the state space, action space,\n",
    "    transition probabilities, rewards, start state, etc.\n",
    "    The agent can take any of the four cardinal directions as an action, getting\n",
    "    a living reward (typically negative in order to incentivize shorter\n",
    "    paths). It can also take the STAY action, in which case it does not receive\n",
    "    the living reward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid, living_reward=-0.01, noise=0):\n",
    "        \"\"\"Initializes the MDP.\n",
    "        grid: A sequence of sequences of spaces, representing a grid of a\n",
    "        certain height and width. See assert_valid_grid for details on the grid\n",
    "        format.\n",
    "        living_reward: The reward obtained when taking any action besides STAY.\n",
    "        noise: Probability that when the agent takes a non-STAY action (that is,\n",
    "        a cardinal direction), it instead moves in one of the two adjacent\n",
    "        cardinal directions.\n",
    "        Raises: AssertionError if the grid is invalid.\n",
    "        \"\"\"\n",
    "        self._assert_valid_grid(grid)\n",
    "\n",
    "        walls = [[space == 'X' for space in row] for row in grid]\n",
    "        rewards, start_state = self._get_rewards_and_start_state(grid)\n",
    "        GridworldMdpNoR.__init__(self, walls, start_state, noise)\n",
    "        self.rewards = rewards\n",
    "        self.living_reward = living_reward\n",
    "\n",
    "    def _assert_valid_grid(self, grid):\n",
    "        \"\"\"Raises an AssertionError if the grid is invalid.\n",
    "        grid:  A sequence of sequences of spaces, representing a grid of a\n",
    "        certain height and width. grid[y][x] is the space at row y and column\n",
    "        x. A space must be either 'X' (representing a wall), ' ' (representing\n",
    "        an empty space), 'A' (representing the start state), or a value v so\n",
    "        that float(v) succeeds (representing a reward).\n",
    "        Often, grid will be a list of strings, in which case the rewards must be\n",
    "        single digit positive rewards.\n",
    "        \"\"\"\n",
    "        height = len(grid)\n",
    "        width = len(grid[0])\n",
    "\n",
    "        # Make sure the grid is not ragged\n",
    "        assert all(len(row) == width for row in grid), 'Ragged grid'\n",
    "\n",
    "        # Borders must all be walls\n",
    "        for y in range(height):\n",
    "            assert grid[y][0] == 'X', 'Left border must be a wall'\n",
    "            assert grid[y][-1] == 'X', 'Right border must be a wall'\n",
    "        for x in range(width):\n",
    "            assert grid[0][x] == 'X', 'Top border must be a wall'\n",
    "            assert grid[-1][x] == 'X', 'Bottom border must be a wall'\n",
    "\n",
    "        def is_float(element):\n",
    "            try:\n",
    "                return float(element) or True\n",
    "            except ValueError:\n",
    "                return False\n",
    "\n",
    "        # An element can be 'X' (a wall), ' ' (empty element), 'A' (the agent),\n",
    "        # or a value v such that float(v) succeeds and returns a float.\n",
    "        def is_valid_element(element):\n",
    "            return element in ['X', ' ', 'A'] or is_float(element)\n",
    "\n",
    "        all_elements = [element for row in grid for element in row]\n",
    "        assert all(is_valid_element(element) for element in all_elements), \\\n",
    "               'Invalid element: must be X, A, blank space, or a number'\n",
    "        assert all_elements.count('A') == 1, \"'A' must be present exactly once\"\n",
    "        floats = [element for element in all_elements if is_float(element)]\n",
    "        assert len(floats) >= 1, 'There must at least one reward square'\n",
    "\n",
    "    def _get_rewards_and_start_state(self, grid):\n",
    "        \"\"\"Extracts the rewards and start state from grid.\n",
    "        Assumes that grid is a valid grid.\n",
    "        grid: A sequence of sequences of spaces, representing a grid of a\n",
    "        certain height and width. See assert_valid_grid for details on the grid\n",
    "        format.\n",
    "        living_reward: The reward obtained each time step (typically negative).\n",
    "        Returns two things -- a dictionary mapping states to rewards, and a\n",
    "        start state.\n",
    "        \"\"\"\n",
    "        rewards = {}\n",
    "        start_state = None\n",
    "        for y in range(len(grid)):\n",
    "            for x in range(len(grid[0])):\n",
    "                if grid[y][x] not in ['X', ' ', 'A']:\n",
    "                    rewards[(x, y)] = float(grid[y][x])\n",
    "                elif grid[y][x] == 'A':\n",
    "                    start_state = (x, y)\n",
    "        return rewards, start_state\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"Get reward for state, action transition.\"\"\"\n",
    "        result = 0\n",
    "        if state in self.rewards:\n",
    "            result += self.rewards[state]\n",
    "        if action != Direction.STAY:\n",
    "            result += self.living_reward\n",
    "        return result\n",
    "\n",
    "    def get_random_start_state(self):\n",
    "        \"\"\"Returns a state that would be a legal start state for an agent.\n",
    "        Avoids walls and reward/exit states.\n",
    "        Returns: Randomly chosen state (x, y).\n",
    "        \"\"\"\n",
    "        y = random.randint(1, self.height - 2)\n",
    "        x = random.randint(1, self.width - 2)\n",
    "        while self.walls[y][x] or (x, y) in self.rewards:\n",
    "            y = random.randint(1, self.height - 2)\n",
    "            x = random.randint(1, self.width - 2)\n",
    "        return (x, y)\n",
    "\n",
    "    def convert_to_numpy_input(self):\n",
    "        \"\"\"Encodes this MDP in a format well-suited for deep models.\n",
    "        Returns three things -- a grid of indicators for whether or not a wall\n",
    "        is present, a grid of reward values (not including living reward), and\n",
    "        the start state (a tuple in the format x, y).\n",
    "        \"\"\"\n",
    "        walls = np.array(self.walls, dtype=int)\n",
    "        rewards = np.zeros([self.height, self.width], dtype=float)\n",
    "        for x, y in self.rewards:\n",
    "            rewards[y, x] = self.rewards[(x, y)]\n",
    "        return walls, rewards, self.start_state\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_input(walls, reward, start_state, noise=0):\n",
    "        \"\"\"Creates the MDP from the format output by convert_to_numpy_input.\n",
    "        See convert_to_numpy_input for the types of the parameters. If\n",
    "        start_state is not provided, some arbitrary blank space is set as the\n",
    "        start state. Assumes that the parameters were returned by\n",
    "        convert_to_numpy_input, and in particular it does not check that they\n",
    "        are valid (for example, it assumes that no space is both a wall and a\n",
    "        reward).\n",
    "        It is *not* the case that calling from_numpy_input on the result of\n",
    "        convert_to_numpy_input will give exactly the same gridworld. In\n",
    "        particular, the living reward and noise will be reset to their default\n",
    "        values.\n",
    "        \"\"\"\n",
    "        def get_elem(x, y):\n",
    "            wall_elem, reward_elem = walls[y][x], reward[y][x]\n",
    "            if wall_elem == 1:\n",
    "                return 'X'\n",
    "            elif reward_elem == 0:\n",
    "                return ' '\n",
    "            else:\n",
    "                return reward_elem\n",
    "\n",
    "        height, width = walls.shape\n",
    "        grid = [[get_elem(x, y) for x in range(width)] for y in range(height)]\n",
    "        x, y = start_state\n",
    "        grid[y][x] = 'A'\n",
    "        return GridworldMdp(grid, noise=noise)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_state(grid, accepted_tokens):\n",
    "        height, width = len(grid), len(grid[0])\n",
    "        current_val = None\n",
    "        while current_val not in accepted_tokens:\n",
    "            y = random.randint(1, height - 2)\n",
    "            x = random.randint(1, width - 2)\n",
    "            current_val = grid[y][x]\n",
    "        return x, y\n",
    "\n",
    "    def without_reward(self):\n",
    "        return GridworldMdpNoR(self.walls, self.start_state, self.noise)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random(height, width, pr_wall, pr_reward):\n",
    "        \"\"\"Generates a random instance of a Gridworld.\n",
    "        Note that based on the generated walls and start position, it may be\n",
    "        impossible for the agent to ever reach a reward.\n",
    "        \"\"\"\n",
    "        grid = [['X'] * width for _ in range(height)]\n",
    "        for y in range(1, height - 1):\n",
    "            for x in range(1, width - 1):\n",
    "                if random.random() < pr_reward:\n",
    "                    grid[y][x] = random.randint(-9, 9)\n",
    "                    # Don't allow 0 rewards\n",
    "                    while grid[y][x] == 0:\n",
    "                        grid[y][x] = random.randint(-9, 9)\n",
    "                elif random.random() >= pr_wall:\n",
    "                    grid[y][x] = ' '\n",
    "\n",
    "        def set_random_position_to(token):\n",
    "            x, y = GridworldMdp.get_random_state(grid, ['X', ' '])\n",
    "            grid[y][x] = token\n",
    "\n",
    "        set_random_position_to(3)\n",
    "        set_random_position_to('A')\n",
    "        return GridworldMdp(grid)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_connected(height, width, num_rewards, noise, goals=None):\n",
    "        \"\"\"Generates a random instance of a Gridworld.\n",
    "        Unlike with generate_random, it is guaranteed that the agent\n",
    "        can reach a reward. However, that reward might be negative.\n",
    "        goals: If not None, dictionary mapping (x, y) positions to rewards.\n",
    "        \"\"\"\n",
    "        def get_random_reward():\n",
    "            result = random.randint(-9, 9)\n",
    "            while result == 0:\n",
    "                result = random.randint(-9, 9)\n",
    "            return result\n",
    "\n",
    "        def generate_goals(start_state):\n",
    "            states = [(x, y) for x in range(1, width-1) for y in range(1, height-1)]\n",
    "            states.remove(start_state)\n",
    "            indices = np.random.choice(len(states), num_rewards, replace=False)\n",
    "            return {states[i] : get_random_reward() for i in indices}\n",
    "\n",
    "        start_state = (width // 2, height // 2)\n",
    "        if goals is None:\n",
    "            goals = generate_goals(start_state)\n",
    "        required_nonwalls = list(goals.keys())\n",
    "        required_nonwalls.append(start_state)\n",
    "\n",
    "        directions = [\n",
    "            Direction.NORTH, Direction.SOUTH, Direction.EAST, Direction.WEST]\n",
    "        grid = [['X'] * width for _ in range(height)]\n",
    "        walls = [(x, y) for x in range(1, width-1) for y in range(1, height-1)]\n",
    "        dsets = DisjointSets([])\n",
    "        first_state = required_nonwalls[0]\n",
    "        for x, y in required_nonwalls:\n",
    "            grid[y][x] = ' '\n",
    "            walls.remove((x, y))\n",
    "            dsets.add_singleton((x, y))\n",
    "\n",
    "        min_free_spots = len(walls) / 2\n",
    "        random.shuffle(walls)\n",
    "        while dsets.get_num_elements() < min_free_spots or not dsets.is_connected():\n",
    "            x, y = walls.pop()\n",
    "            grid[y][x] = ' '\n",
    "            dsets.add_singleton((x, y))\n",
    "            for direction in directions:\n",
    "                newx, newy = Direction.move_in_direction((x, y), direction)\n",
    "                if dsets.contains((newx, newy)):\n",
    "                    dsets.union((x, y), (newx, newy))\n",
    "\n",
    "        grid[height // 2][width // 2] = 'A'\n",
    "        for x, y in goals.keys():\n",
    "            grid[y][x] = goals[(x, y)]\n",
    "\n",
    "        return GridworldMdp(grid, noise=noise)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Returns a string representation of this grid world.\n",
    "        The returned string has a line for every row, and each space is exactly\n",
    "        one character. These are encoded in the same way as the grid input to\n",
    "        the constructor -- walls are 'X', empty spaces are ' ', the start state\n",
    "        is 'A', and rewards are their own values. However, rewards like 3.5 or\n",
    "        -9 cannot be represented with a single character. Such rewards are\n",
    "        encoded as 'R' (if positive) or 'N' (if negative).\n",
    "        \"\"\"\n",
    "        def get_char(x, y):\n",
    "            if self.walls[y][x]:\n",
    "                return 'X'\n",
    "            elif (x, y) in self.rewards:\n",
    "                reward = self.rewards[(x, y)]\n",
    "                # Convert to an int if it would not lose information\n",
    "                reward = int(reward) if int(reward) == reward else reward\n",
    "                posneg_char = 'R' if reward >= 0 else 'N'\n",
    "                reward_str = str(reward)\n",
    "                return reward_str if len(reward_str) == 1 else posneg_char\n",
    "            elif (x, y) == self.get_start_state():\n",
    "                return 'A'\n",
    "            else:\n",
    "                return ' '\n",
    "\n",
    "        def get_row_str(y):\n",
    "            return ''.join([get_char(x, y) for x in range(self.width)])\n",
    "\n",
    "        return '\\n'.join([get_row_str(y) for y in range(self.height)])\n",
    "\n",
    "class Direction(object):\n",
    "    \"\"\"A class that contains the five actions available in Gridworlds.\n",
    "    Includes definitions of the actions as well as utility functions for\n",
    "    manipulating them or applying them.\n",
    "    \"\"\"\n",
    "    NORTH = (0, -1)\n",
    "    SOUTH = (0, 1)\n",
    "    EAST  = (1, 0)\n",
    "    WEST  = (-1, 0)\n",
    "    STAY = (0, 0)\n",
    "    INDEX_TO_DIRECTION = [NORTH, SOUTH, EAST, WEST, STAY]\n",
    "    DIRECTION_TO_INDEX = { a:i for i, a in enumerate(INDEX_TO_DIRECTION) }\n",
    "    ALL_DIRECTIONS = INDEX_TO_DIRECTION\n",
    "\n",
    "    @staticmethod\n",
    "    def move_in_direction(point, direction):\n",
    "        \"\"\"Takes a step in the given direction and returns the new point.\n",
    "        point: Tuple (x, y) representing a point in the x-y plane.\n",
    "        direction: One of the Directions, except not Direction.STAY or\n",
    "                   Direction.SELF_LOOP.\n",
    "        \"\"\"\n",
    "        x, y = point\n",
    "        dx, dy = direction\n",
    "        return (x + dx, y + dy)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_adjacent_directions(direction):\n",
    "        \"\"\"Returns the directions within 90 degrees of the given direction.\n",
    "        direction: One of the Directions, except not Direction.STAY.\n",
    "        \"\"\"\n",
    "        if direction in [Direction.NORTH, Direction.SOUTH]:\n",
    "            return [Direction.EAST, Direction.WEST]\n",
    "        elif direction in [Direction.EAST, Direction.WEST]:\n",
    "            return [Direction.NORTH, Direction.SOUTH]\n",
    "        raise ValueError('Invalid direction: %s' % direction)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_number_from_direction(direction):\n",
    "        return Direction.DIRECTION_TO_INDEX[direction]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_direction_from_number(number):\n",
    "        return Direction.INDEX_TO_DIRECTION[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5685f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50cc52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution(object):\n",
    "    \"\"\"Represents a probability distribution.\n",
    "    The distribution is stored in a canonical form where items are mapped to\n",
    "    their probabilities. The distribution is always normalized (so that the\n",
    "    probabilities sum to 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, probability_mapping):\n",
    "        # Convert to a list so that we aren't iterating over the dictionary and\n",
    "        # removing at the same time\n",
    "        for key in list(probability_mapping.keys()):\n",
    "            prob = probability_mapping[key]\n",
    "            if prob == 0:\n",
    "                del probability_mapping[key]\n",
    "            elif prob < 0:\n",
    "                raise ValueError('Cannot have negative probability!')\n",
    "\n",
    "        assert len(probability_mapping) > 0\n",
    "        self.dist = probability_mapping\n",
    "        self.normalize()\n",
    "\n",
    "    def factor(self, key, factor):\n",
    "        \"\"\"Updates the probability distribution as though we see evidence that\n",
    "        is `factor` times more likely for `key` than for any other key.\"\"\"\n",
    "        self.dist[key] *= factor\n",
    "        self.normalize()\n",
    "\n",
    "    def normalize(self):\n",
    "        Z = float(sum(self.dist.values()))\n",
    "        for key in list(self.dist.keys()):\n",
    "            self.dist[key] /= Z\n",
    "\n",
    "    def sample(self):\n",
    "        keys, probabilities = zip(*self.dist.items())\n",
    "        return keys[np.random.choice(np.arange(len(keys)), p=probabilities)]\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self.dist.copy()\n",
    "\n",
    "    def as_numpy_array(self, fn=None, length=None):\n",
    "        if fn is None:\n",
    "            fn = lambda x: x\n",
    "        keys = list(self.dist.keys())\n",
    "        numeric_keys = [fn(key) for key in keys]\n",
    "        if length is None:\n",
    "            length = max(numeric_keys) + 1\n",
    "\n",
    "        result = np.zeros(length)\n",
    "        for key, numeric_key in zip(keys, numeric_keys):\n",
    "            result[numeric_key] = self.dist[key]\n",
    "        return result\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.dist == other.dist\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dist)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Distribution(%s)' % repr(self.dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c425bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(-1, 0): 1.0}\n"
     ]
    }
   ],
   "source": [
    "grid = ['XXXXXXXXX',\n",
    "        'X9X6X   X',\n",
    "        'X X X XXX',\n",
    "        'X  A   2X',\n",
    "        'XXXXXXXXX']\n",
    "n, s, e, w, stay = Direction.ALL_DIRECTIONS\n",
    "\n",
    "agent = OptimalAgent(gamma=0.95, num_iters=20)\n",
    "mdp = GridworldMdp(grid, living_reward=-0.1)\n",
    "agent.set_mdp(mdp)\n",
    "start_state = mdp.get_start_state()\n",
    "\n",
    "# Action distribution\n",
    "action_dist = agent.get_action_distribution(start_state)\n",
    "\n",
    "print(action_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c8233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
